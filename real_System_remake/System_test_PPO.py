# import tensorflow as tf
import copy
import gc
import json
from fileinput import filename
from datetime import datetime
import numpy as np
import os
import random
import torch

import swanlab as wandb
from Agent.Config_PPO import Config_PPO
from real_System_remake.Bank_config import Bank_config
from real_System_remake.Enterprise_config import Enterprise_config
from real_System_remake.Environment import Environment
from real_System_remake.ppo_bank import bank_nnu
from real_System_remake.ppo_enterprise import enterprise_nnu
import torch
from torch.distributions import Normal
import torch.nn.functional as F
from swanlab.plugin.notification import EmailCallback
use_wandb = True
use_rbtree = False
lim_day = 100
# seed =125
enterprise_ppo_config = Config_PPO(
    scope='',
    state_dim=0,
    action_dim=4,
    hidden_dim=64,
)

bank_ppo_config = Config_PPO(
    scope='',
    state_dim=0,
    action_dim=2,
    hidden_dim=64,

)

bank_config = Bank_config(
    name='bank1',
    fund=2000,
    fund_rate=1,
    fund_increase=0.1,
    debt_time=5
)

enterprise_config = Enterprise_config(
    name='',
    output_name='',
    price=8.0, intention=5.0)

# ä¸¤ä¸ªä¼ä¸šï¼Œä¸€ä¸ªç”Ÿäº§Kï¼Œä¸€ä¸ªç”Ÿäº§L
enterprise_add_list = {
    'production1': 'K',
    'consumption1': 'L'
}


class System:
    def __init__(self):
        self.env = Environment(name='PPO', lim_day=lim_day)

        for key in enterprise_add_list:
            config = copy.deepcopy(enterprise_config)
            config.name = key
            config.output_name = enterprise_add_list[key]
            self.env.add_enterprise_agent(config=config)
        self.env.add_bank(bank_config)
        self.env.add_enterprise_thirdmarket(name='production_thirdMarket', output_name='K', price=100)
        self.env.add_enterprise_thirdmarket(name='consumption_thirdMarket', output_name='L', price=100)

        self.env.init()
        # self.epiday=0 #å›åˆæ•°ï¼Œåœ¨ç®—æ³•å¤ªåƒåœ¾çš„æ—¶å€™å¯ä»¥æå‰ç»“æŸã€‚
        self.e_execute = self.env.get_enterprise_execute()
        self.b_execute = self.env.get_bank_execute()
        self.execute = self.e_execute + self.b_execute
        self.Agent = {}
        for key in self.execute:
            self.Agent[key] = None
        # è¯„ä¼°é…ç½®é¡¹
        self.eval_interval_steps = 5000
        self.eval_episodes = 5
        self.eval_deterministic = False   # é»˜è®¤ False

    #æ–°å¢â€œæ„å»ºç‹¬ç«‹ç¯å¢ƒâ€çš„å‡½æ•°
    def _build_env(self, name: str):
        env = Environment(name=name, lim_day=lim_day)

        for key in enterprise_add_list:
            config = copy.deepcopy(enterprise_config)
            config.name = key
            config.output_name = enterprise_add_list[key]
            env.add_enterprise_agent(config=config)

        env.add_bank(bank_config)
        env.add_enterprise_thirdmarket(name='production_thirdMarket', output_name='K', price=100)
        env.add_enterprise_thirdmarket(name='consumption_thirdMarket', output_name='L', price=100)

        env.init()
        return env

    #æ–°å¢çª—å£å¿«ç…§ä¸æ¢å¤å‡½æ•°
    def _snapshot_agent_windows(self):
        snap = {}
        for k, agent in self.Agent.items():  #kæ˜¯æ™ºèƒ½ä½“åå­—
            if hasattr(agent, "get_window_state"):
                snap[k] = agent.get_window_state()
            else:
                snap[k] = None
        return snap

    def _restore_agent_windows(self, snap):
        for k, agent in self.Agent.items():
            if hasattr(agent, "set_window_state"):
                agent.set_window_state(snap.get(k))

    def evaluate_current_policy(self, steps: int,eval_episodes: int =50, deterministic: bool = False):
        """
        æ¯æ¬¡è°ƒç”¨ï¼šç”¨ç‹¬ç«‹ eval_env è·‘ self.eval_episodes å›åˆã€‚
        è®°å½•ï¼š
          - å¹³å‡å­˜æ´»å¤©æ•°
          - ç ´äº§ç‡ï¼ˆterminated å æ¯”ï¼‰
          - ç»¼åˆæ”¶ç›Šï¼šæŒ‰ä½ é€‰çš„ A å£å¾„ï¼Œåˆ†åˆ«è®°å½• enterprise çš„ total_reward['eval_business'] / ['business']ï¼Œä»¥åŠ bank çš„ total_rewardï¼ˆå°½é‡å…¼å®¹ï¼‰
        """
        import numpy as np

        # 1) ä¿å­˜è®­ç»ƒçª—å£ï¼ˆå…³é”®ï¼‰
        window_snap = self._snapshot_agent_windows()

        # 2) è¯„ä¼°å‰æ¸…ç©ºçª—å£ï¼ˆæ¯ä¸ª eval episode éƒ½ä»å¹²å‡€çª—å£å¼€å§‹ï¼‰
        for agent in self.Agent.values():
            if hasattr(agent, "reset_window"):
                agent.reset_window()

        for agent in self.Agent.values():
            if hasattr(agent,"enterprise"):
                agent.enterprise.actor.eval()
                agent.enterprise.critic.eval()
            if hasattr(agent,"bank"):
                agent.bank.actor.eval()
                agent.bank.critic.eval()

        # 3) åˆ›å»ºç‹¬ç«‹è¯„ä¼°ç¯å¢ƒï¼ˆä¸è®­ç»ƒç¯å¢ƒå®Œå…¨åˆ†å¼€ï¼‰
        eval_env = self._build_env(name=f"PPO_eval_at_{steps}")

        survival_days = []
        terminated_count = 0
        truncated_count = 0
        # æ¯ä¸ªä¸»ä½“ä¸€ä¸ª dictï¼Œé‡Œé¢å­˜æ¯ä¸ª episode çš„æ”¶ç›Šï¼ˆåç»­å–å‡å€¼/æ–¹å·®ï¼‰
        per_agent = {}

        for target_name in self.e_execute:
            per_agent[target_name] = {
                "eval_business": [],
                # "business": [],
            }
        for target_name in self.b_execute:
            per_agent[target_name] = {
                "WNDB": [],  # é“¶è¡Œåˆ©æ¶¦
            }

            # ========= 4) å¼€å§‹è¯„ä¼°å›åˆ =========
        for ep in range(eval_episodes):
            state = eval_env.reset()
            done = False

            # æ¯ä¸ªè¯„ä¼°å›åˆå¼€å§‹ï¼Œä¹Ÿæ¸…ç©ºçª—å£ï¼Œé¿å…è·¨å›åˆæ³„æ¼
            for agent in self.Agent.values():
                if hasattr(agent, "reset_window"):
                    agent.reset_window()

            while not done:
                action = {}

                # ä¼ä¸šåŠ¨ä½œ
                for k in self.e_execute:
                    if deterministic:
                        act = self.Agent[k].choose_action_deterministic(state[k])
                    else:
                        act, _, _, _ = self.Agent[k].choose_action(state[k])
                    action[k] = act

                # é“¶è¡ŒåŠ¨ä½œ
                for k in self.b_execute:
                    if deterministic:
                        act = self.Agent[k].choose_action_deterministic(state[k])
                    else:
                        act, _, _, _ = self.Agent[k].choose_action(state[k])
                    action[k] = act

                eval_env.step(action)
                next_state, reward, done, info = eval_env.observe()
                state = next_state

            # ========= 5) å›åˆç»“æŸç»Ÿè®¡ =========
            is_terminated = bool(info.get("terminated", False))  # ç ´äº§ï¼ˆè‡ªç„¶ç»ˆæ­¢ï¼‰
            is_truncated = bool(info.get("truncated", False))  # åˆ°è¾¾ lim_day æˆªæ–­

            # æ¨èäº’æ–¥å½’å› ï¼šç ´äº§ä¼˜å…ˆï¼›å¦åˆ™æ‰ç®—æˆªæ–­
            if is_terminated:
                terminated_count += 1
            elif is_truncated:
                truncated_count += 1

            # å­˜æ´»å¤©æ•°ï¼ˆå›åˆçº§ï¼‰
            survival_days.append(eval_env.day)

            # åˆ†ä¼ä¸šæ”¶ç›Šï¼šç›´æ¥è¯» episode æœ«çš„ total_reward
            for target_name in self.e_execute:
                total_reward = eval_env.Enterprise[target_name].total_reward  # dict: {'eval_business':..., 'business':...}
                per_agent[target_name]["eval_business"].append(100 * total_reward["eval_business"])

            # åˆ†é“¶è¡Œæ”¶ç›Šï¼šä¼˜å…ˆ WNDBï¼Œå¦åˆ™ sum(values)
            for target_name in self.b_execute:
                trb = eval_env.Bank[target_name].total_reward["WNDB"] * 100
                per_agent[target_name]["WNDB"].append(trb)

            # ========= 6) æ±‡æ€»ç»Ÿè®¡ =========
        survival = np.array(survival_days, dtype=np.float32)

        result = {
            "steps": int(steps),
            # "eval_episodes": int(eval_episodes),
            # "deterministic": bool(deterministic),

            "avg_survival_days": float(survival.mean()),

            "terminated_count": int(terminated_count),
            "truncated_count": int(truncated_count),
            "bankruptcy_rate": float(terminated_count / max(1, eval_episodes)),
            "truncated_rate": float(truncated_count / max(1, eval_episodes)),

            "agents": {}
        }

        # ä¼ä¸šåˆ†åˆ«æ±‡æ€»
        for target_name in self.e_execute:
            eb = np.array(per_agent[target_name]["eval_business"], dtype=np.float32)
            result["agents"][target_name] = {
                "avg_total_eval_business": float(eb.mean()),
            }

        # é“¶è¡Œåˆ†åˆ«æ±‡æ€»
        for target_name in self.b_execute:
            bt = np.array(per_agent[target_name]["WNDB"], dtype=np.float32)
            result["agents"][target_name] = {
                "avg_total_reward": float(bt.mean()),
            }

        # ========= 7) wandb/swanlab è®°å½•ï¼ˆæŒ‰ä¸»ä½“åˆ†åˆ«æ‰“ç‚¹ï¼‰ =========
        # å…¨å±€æŒ‡æ ‡
        wandb_payload = {
            "eval/avg_survival_days": result["avg_survival_days"],
            "eval/bankruptcy_rate": result["bankruptcy_rate"],
        }

        # åˆ†ä¼ä¸š
        for target_name in self.e_execute:
            wandb_payload[f"eval/{target_name}/avg_total_eval_business"] = result["agents"][target_name]["avg_total_eval_business"]

        # åˆ†é“¶è¡Œ
        for target_name in self.b_execute:
            wandb_payload[f"eval/{target_name}/avg_total_reward"] = result["agents"][target_name]["avg_total_reward"]


        # ç”¨è®­ç»ƒæ­¥æ•°å¯¹é½æ¨ªè½´
        wandb.log(wandb_payload, step=int(steps))

        # ========= 9) æ¢å¤è®­ç»ƒçª—å£ï¼ˆå…³é”®ï¼šç»§ç»­æœªå®Œæˆè®­ç»ƒå›åˆï¼‰ =========
        self._restore_agent_windows(window_snap)
        # è¯„ä¼°åï¼šåˆ‡å› trainï¼ˆç»§ç»­è®­ç»ƒå¿…é¡»åšï¼‰
        for agent in self.Agent.values():
            if hasattr(agent, "enterprise"):
                agent.enterprise.actor.train()
                agent.enterprise.critic.train()
            if hasattr(agent, "bank"):
                agent.bank.actor.train()
                agent.bank.critic.train()
        return result

    def run(self,seed=None):
        config = Config_PPO(scope='', state_dim=0, action_dim=0, hidden_dim=0)
        wandb.init(project="CL_learn", workspace="wx829", config={
            "random_seed": seed,
            "is_rms_state": config.is_rms_state,
            "is_rms_reward": config.is_rms_reward,
            "max_training_steps": config.MAX_TRAINING_STEPS,
            "total_step": config.total_step,
            "learning_rate_actor_enterprise": config.LEARNING_RATE_AC_Enterprise,
            "learning_rate_actor_bank": config.LEARNING_RATE_AC_Bank,
            "learning_rate_critic_enterprise": config.LEARNING_RATE_C_Enterprise,
            "learning_rate_critic_bank": config.LEARNING_RATE_C_Bank,
            "entropyRC_Enterprise": config.entropyRC_Enterprise,
            "entropyRC_Bank": config.entropyRC_Bank,
            "clip_range": config.CLIP_RANGE,
            "epoch": config.N_EPOCHS,
            "mini_batch": config.MINI_BATCH_SIZE,
            "update_timestep": config.UPDATE_TIMESTEP,
            "total_update": config.MAX_TRAINING_STEPS / config.UPDATE_TIMESTEP,
            "lim-day": lim_day,
            "gamma":config.GAMMA,
            "lambda":config.LAMDA,
            #transformerå‚æ•°
            "trans_seq_len":config.seq_len,
            "trans_n_heads":config.n_heads,
            "trans_n_layers":config.n_layers
        })
        # 1. PPO è¶…å‚æ•°
        update_timestep = config.UPDATE_TIMESTEP
        # max_training_timesteps = config.MAX_TRAINING_STEPS
        total_step =config.total_step
        # 2. åˆå§‹åŒ–æ™ºèƒ½ä½“
        _temp_state = self.env.reset()
        for target_key in self.e_execute:
            if self.Agent[target_key] is None:
                config = copy.deepcopy(enterprise_ppo_config)
                config.set_scope(target_key)
                config.set_seed(seed)
                config.set_state_dim(len(_temp_state[target_key]))
                self.Agent[target_key] = enterprise_nnu(config)
        for target_key in self.b_execute:
            if self.Agent[target_key] is None:
                config = copy.deepcopy(bank_ppo_config)
                config.set_scope(target_key)
                config.set_seed(seed)
                config.set_state_dim(len(_temp_state[target_key]))
                self.Agent[target_key] = bank_nnu(config)

       # self.load_actor_only()
        # 3. å¼€å§‹è®­ç»ƒå¾ªç¯
        state = self.env.reset()
        time_step = 0
        update_num= 0
        episode_num = 1

        while time_step < total_step:

            # --- æ•°æ®æ”¶é›†é˜¶æ®µ ---
            for _ in range(update_timestep):
                time_step += 1
                if time_step % self.eval_interval_steps == 0:
                    print("start to evalute")
                    self.evaluate_current_policy(steps=time_step)
                action,log_prob,mus,sigmas = {}, {},{},{}

                for target_key in self.e_execute:
                    act, lp ,mu,sigma= self.Agent[target_key].choose_action(state[target_key])
                    action[target_key], log_prob[target_key],mus[target_key],sigmas[target_key] = act, lp,mu,sigma
                for target_key in self.b_execute:
                    act, lp ,mu,sigma= self.Agent[target_key].choose_action(state[target_key])
                    action[target_key], log_prob[target_key],mus[target_key],sigmas[target_key] = act, lp,mu,sigma

                self.env.step(action)
                next_state, reward, done_env,info = self.env.observe()

                # NEW: ä¸ºæ¯ä¸ª agent è®¡ç®— next_value = V(next_state)
                next_v = {}
                for k in self.e_execute:
                    next_v[k] = self.Agent[k].get_value(next_state[k])
                for k in self.b_execute:
                    next_v[k] = self.Agent[k].get_value(next_state[k])

                # NEW: åŒæ©ç 
                is_terminated = bool(info.get('terminated', done_env))  # è‡ªç„¶ç»ˆæ­¢æ‰æˆªæ–­ bootstrap
                nonterminal = 0 if done_env else 1  # ç»“æŸ(terminated æˆ– truncated)åˆ™æ–­å¼€ GAE é€’æ¨

                # CHANGED: å­˜ transitionï¼ˆå« next_value ä¸ nonterminalï¼‰
                for target_key in self.e_execute:
                    self.Agent[target_key].store_transition(
                        state[target_key],
                        mus[target_key],
                        sigmas[target_key],
                        action[target_key],
                        log_prob[target_key],
                        reward[target_key]['business'],
                        is_terminated,
                        next_v[target_key],
                        nonterminal,
                    )
                for target_key in self.b_execute:
                    self.Agent[target_key].store_transition(
                        state[target_key],
                        mus[target_key],
                        sigmas[target_key],
                        action[target_key],
                        log_prob[target_key],
                        reward[target_key]['WNDB'],
                        is_terminated,
                        next_v[target_key],
                        nonterminal,
                    )

                state = next_state

                if done_env:
                    print(f"Episode {episode_num} finished. Total timesteps: {time_step}")
                    #1. ç¯å¢ƒé‡ç½®
                    state = self.env.reset()
                    # 2. ã€å…³é”®ã€‘é‡ç½®æ‰€æœ‰æ™ºèƒ½ä½“çš„å†å²çª—å£
                    for target_key in self.e_execute:
                        self.Agent[target_key].reset_window()
                    for target_key in self.b_execute:
                        self.Agent[target_key].reset_window()
                    episode_num += 1


            # --- å­¦ä¹ é˜¶æ®µ ---
            print(f"--- Timestep {time_step}. Updating policies... ---")

            for agent_key, agent in self.Agent.items():
                agent.learn(state[agent_key])
                agent.clear_memory()

            if use_wandb and update_num % 5 ==0 :
                critic_bank, actor_bank, avg_entropy_bank, clip_fraction_bank = self.Agent['bank1'].log()
                critic_production1, actor_production1, avg_entropy_production1, clip_fraction_production1 = self.Agent[
                    'production1'].log()
                critic_consumption1, actor_consumption1, avg_entropy_consumption1, clip_fraction_consumption1 = \
                    self.Agent['consumption1'].log()
                wandb.log({'actor_loss/bank1': actor_bank})
                wandb.log({'actor_loss/production1': actor_production1})
                wandb.log({'actor_loss/consumption1': actor_consumption1})

                wandb.log({'critic_loss/bank': critic_bank})
                wandb.log({'critic_loss/production1': critic_production1})
                wandb.log({'critic_loss/consumption1': critic_consumption1})

                wandb.log({'avg_entropy/bank': avg_entropy_bank})
                wandb.log({'avg_entropy/production1': avg_entropy_production1})
                wandb.log({'avg_entropy/consumption1': avg_entropy_consumption1})

                wandb.log({'clip_fraction/bank': clip_fraction_bank})
                wandb.log({'clip_fraction/production1': clip_fraction_production1})
                wandb.log({'clip_fraction/consumption1': clip_fraction_consumption1})

            print("--- Update finished. ---")
            update_num += 1
        wandb.finish()
        # self.env.finish()

    @staticmethod
    def set_seed(seed):
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        os.environ["PYTHONHASHSEED"] = str(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        print(f"[INFO] Seed set to {seed}")


    def collect_state_statistics(self, episodes=20):
        """
        ç”¨æœ€ç»ˆç­–ç•¥é‡‡æ ·è‹¥å¹²æ¡è½¨è¿¹ï¼Œè¿”å›æ¯ä¸ª agent æ¯ç»´ mean å’Œ stdï¼ˆflatten åï¼‰ã€‚
        ç»“æœæ ¼å¼ï¼šdict(agent_key -> {'mean': np.array, 'std': np.array})
        """
        all_states_per_agent = {k: [] for k in self.Agent.keys()}

        for ep in range(episodes):
            state = self.env.reset()
            done = False
            while not done:
                # éšæœº/ç¡®å®šåœ°ç”¨å½“å‰è®­ç»ƒå¥½ç­–ç•¥é‡‡æ ·åŠ¨ä½œï¼ˆå»ºè®®ä½¿ç”¨è®­ç»ƒæ—¶çš„é‡‡æ ·è¡Œä¸ºï¼‰
                action = {}
                for target_key in self.e_execute:
                    act, _ = self.Agent[target_key].choose_action(state[target_key])
                    action[target_key] = act

                for target_key in self.b_execute:
                    act = self.Agent[target_key].choose_action(state[target_key])
                    action[target_key] = act
                self.env.step(action)
                next_state, reward, done = self.env.observe()
                # æ”¶é›†æ¯ä¸ª agent çš„ stateï¼ˆæŒ‰ list/array æ ¼å¼ï¼‰
                for key in self.Agent.keys():
                    arr = np.array(next_state[key], dtype=np.float32).ravel()
                    all_states_per_agent[key].append(arr)
                state = next_state

        stats = {}
        for key, list_of_states in all_states_per_agent.items():
            if len(list_of_states) == 0:
                continue
            S = np.stack(list_of_states, axis=0)  # [T, D]
            stats[key] = {'mean': S.mean(axis=0), 'std': S.std(axis=0)}
        return stats

    def add_multiplicative_noise_to_state_vector(vec, std_vec, alpha):
        """
        vec: 1D numpy array (state flattened)
        std_vec: 1D numpy array same shape (per-dim std from collect)
        alpha: scalar factor (noise level relative to std)
        è¿”å›ï¼š noisy 1D array
        ä½¿ç”¨ä¹˜æ³•å™ªå£° s' = s * (1 + eps), eps ~ N(0, alpha * std_rel)
        è¿™é‡Œç”¨ç›¸å¯¹ std: std_rel = std / (|mean|+eps) ä¹Ÿå¯ç›´æ¥ç”¨ stdã€‚
        """
        eps_small = 1e-8
        # è‹¥ std_vec ä¸­æœ‰ 0ï¼Œé€€åŒ–åˆ°ä¸€ä¸ªå°å¸¸æ•°
        noise_sigma = alpha * (std_vec + eps_small)
        eps = np.random.normal(0.0, noise_sigma, size=vec.shape)
        return (vec * (1.0 + eps)).astype(np.float32)

    def reset_with_noise(self, noise_scale=0.2):
        """
        å¸¦åˆå§‹çŠ¶æ€æ‰°åŠ¨çš„ reset
        å¯¹çŠ¶æ€å‘é‡ä¸­çš„æ¯ä¸ª agent æ·»åŠ å¾®å°é«˜æ–¯å™ªå£°
        """
        state = self.env.reset()
        noisy_state = {}

        for key, value in state.items():
            # å¦‚æœæ˜¯ listï¼Œå…ˆè½¬æˆ np.array
            if isinstance(value, list):
                value = np.array(value, dtype=np.float32)

            # å¦‚æœæ˜¯ numpy æ•°ç»„ï¼Œæ·»åŠ å™ªå£°
            if isinstance(value, np.ndarray):
                noise = np.random.normal(0, noise_scale, size=value.shape)
                noisy_value = value + noise
                noisy_state[key] = noisy_value.tolist()  # è½¬å› listï¼Œä¿è¯ç¯å¢ƒå…¼å®¹
            else:
                # å¯¹éæ•°ç»„ï¼ˆå¦‚æ ‡é‡ã€å­—å…¸ï¼‰ä¿æŒåŸæ ·
                noisy_state[key] = value

        return noisy_state

    def save_actors(self, save_dir="actors_only"):
        """
        ä¿å­˜æ‰€æœ‰æ™ºèƒ½ä½“çš„ Actor å‚æ•°ï¼ˆä»…ç”¨äºè¯„ä¼°ï¼‰
        """
        save_dir = save_dir + "_" +"lim_day="+str(lim_day) +"_seed="+str(seed)
        os.makedirs(save_dir, exist_ok=True)
        for target_key in self.e_execute:
            agent = self.Agent[target_key]
            filename = f"{save_dir}/{agent.scope}_actor.pt"
            torch.save(agent.enterprise.actor.state_dict(), filename)
            print(f"[ğŸ¯] å·²ä¿å­˜ {agent.scope} çš„ actor è‡³ {filename}")

        for target_key in self.b_execute:
            agent = self.Agent[target_key]
            filename = f"{save_dir}/{agent.scope}_actor.pt"
            torch.save(agent.bank.actor.state_dict(), filename)
            print(f"[ğŸ¯] å·²ä¿å­˜ {agent.scope} çš„ actor è‡³ {filename}")

    def load_actor_only(self, save_dir = "actors_only_lim_day=150_seed=451"):
        for target_key in self.e_execute:
            agent = self.Agent[target_key]
            path = os.path.join(save_dir, f"{agent.scope}_actor.pt")
            if os.path.exists(path):
                agent.enterprise.actor.load_state_dict(torch.load(path))
                agent.enterprise.actor.train()
                print(f"[ğŸ¯] åŠ è½½ actor: {agent.scope}")

        for target_key in self.b_execute:
            agent = self.Agent[target_key]
            path = os.path.join(save_dir, f"{agent.scope}_actor.pt")
            if os.path.exists(path):
                agent.bank.actor.load_state_dict(torch.load(path))
                agent.bank.actor.train()
                print(f"[ğŸ¯] åŠ è½½ actor: {agent.scope}")

if __name__ == '__main__':
    # for i in range(3):
    seeds_to_run=[105]
    for seed in seeds_to_run:
        print(f"=== å¯åŠ¨ seed={seed} çš„å®éªŒ ===")
        system = System()
        system.run(seed=seed)
        # system.save_actors()
        # system.evaluate_policy(episodes=500, deterministic=False, threshold=180)
        del system
        gc.collect()
        # æ¸…ç©ºè®¡ç®—å›¾
        torch.nn.Module.dump_patches = True
        torch.cuda.empty_cache()
        print(f"=== seed={seed} å®éªŒç»“æŸ ===\n")

    # tf.reset_default_graph()
